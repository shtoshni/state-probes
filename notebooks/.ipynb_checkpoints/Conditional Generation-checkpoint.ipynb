{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartConfig, BartTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/shtoshni/Research/state-probes/models/epochs_100_patience_5_state_0.5_all/best/model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = BartForConditionalGeneration\n",
    "config_class = BartConfig\n",
    "model_fp = 'facebook/bart-base'\n",
    "\n",
    "config = config_class.from_pretrained(model_fp)        \n",
    "model = model_class(config)\n",
    "\n",
    "tokenizer = BartTokenizerFast.from_pretrained(model_fp)\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "model_dict = checkpoint['model']\n",
    "\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(model_dict)\n",
    "except RuntimeError:\n",
    "    PROBE_START = '[PROBE_START]'\n",
    "    PROBE_END = '[PROBE_END]'\n",
    "    tokenizer.add_special_tokens({\n",
    "        'additional_special_tokens': [PROBE_START, PROBE_END]\n",
    "    })\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(add_state='all', base_model_dir='models', batchsize=24, best_model_dir='models/epochs_20_patience_5_state_0.25_all/best', best_model_path='models/epochs_20_patience_5_state_0.25_all/best/model.pt', encode_init_state='NL', epochs=20, eval=False, lr=1e-05, model_dir='models/epochs_20_patience_5_state_0.25_all', model_path='models/epochs_20_patience_5_state_0.25_all/model.pt', patience=5, rap_prob=0.25, seed=45, use_state_loss=True)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PROBE_END]']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([len(tokenizer) - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_english_phrase = \"The fifth beaker has 2 orange \" \n",
    "batch = tokenizer(example_english_phrase, return_tensors='pt')\n",
    "generated_ids = model.generate(batch['input_ids'], \n",
    "                               pad_token_id=tokenizer.pad_token_id, \n",
    "                               eos_token_id=tokenizer.eos_token_id, #len(tokenizer) - 1, \n",
    "                               bos_token_id=tokenizer.bos_token_id,  \n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s><s>[PROBE_START]  the fourth beaker has 2 orange [PROBE_END] pour the fifth beaker into</s>']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rap_nlp] *",
   "language": "python",
   "name": "conda-env-rap_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
