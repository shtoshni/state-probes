{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/share/data/speech/shtoshni/research/state-probes/models\"\n",
    "seed = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"multitask_size_base_epochs_100_patience_10_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.1_all_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.1_random_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.1_targeted_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.2_all_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.2_random_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.2_targeted_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.3_all_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.3_random_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.3_targeted_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.4_all_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.4_random_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.4_targeted_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.5_all_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.5_random_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.5_targeted_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.6_all_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.6_random_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.6_targeted_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.7_all_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.7_random_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.7_targeted_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.8_all_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.8_random_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.8_targeted_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.9_all_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.9_random_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_0.9_targeted_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_1.0_all_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_1.0_random_text_seed_100\", \n",
      "\"multitask_size_base_epochs_100_patience_10_state_1.0_targeted_text_seed_100\", \n"
     ]
    }
   ],
   "source": [
    "model_paths = glob.glob(path.join(base_dir, f\"multitask_*seed_{seed}\"))\n",
    "\n",
    "# print(model_paths)\n",
    "\n",
    "model_names = [path.split(model_path.rstrip(\"/\"))[1] for model_path in model_paths]\n",
    "model_names = sorted(model_names)\n",
    "for model_name in model_names:\n",
    "    print(f'\"{model_name}\", ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multitask_size_base_epochs_100_patience_10_state_0.1_all_text_seed_100\n",
      "[4294, 4302]\n",
      "[5, 2558]\n",
      "1.083\n",
      "multitask_size_base_epochs_100_patience_10_state_0.1_random_text_seed_100\n",
      "[4049, 4302]\n",
      "[1015, 2558]\n",
      "1.070\n",
      "multitask_size_base_epochs_100_patience_10_state_0.1_targeted_text_seed_100\n",
      "[3880, 4302]\n",
      "[115, 2558]\n",
      "1.076\n",
      "multitask_size_base_epochs_100_patience_10_state_0.2_all_text_seed_100\n",
      "[4297, 4302]\n",
      "[13, 2558]\n",
      "1.085\n",
      "multitask_size_base_epochs_100_patience_10_state_0.2_random_text_seed_100\n",
      "[4095, 4302]\n",
      "[1257, 2558]\n",
      "1.074\n",
      "multitask_size_base_epochs_100_patience_10_state_0.2_targeted_text_seed_100\n",
      "[4030, 4302]\n",
      "[837, 2558]\n",
      "1.079\n",
      "multitask_size_base_epochs_100_patience_10_state_0.3_all_text_seed_100\n",
      "[4298, 4302]\n",
      "[139, 2558]\n",
      "1.084\n",
      "multitask_size_base_epochs_100_patience_10_state_0.3_random_text_seed_100\n",
      "[3724, 4302]\n",
      "[1337, 2558]\n",
      "1.078\n",
      "multitask_size_base_epochs_100_patience_10_state_0.3_targeted_text_seed_100\n",
      "[3864, 4302]\n",
      "[1206, 2558]\n",
      "1.066\n",
      "multitask_size_base_epochs_100_patience_10_state_0.4_all_text_seed_100\n",
      "[4291, 4302]\n",
      "[421, 2558]\n",
      "1.086\n",
      "multitask_size_base_epochs_100_patience_10_state_0.4_random_text_seed_100\n",
      "[3975, 4302]\n",
      "[1362, 2558]\n",
      "1.076\n",
      "multitask_size_base_epochs_100_patience_10_state_0.4_targeted_text_seed_100\n",
      "[3974, 4302]\n",
      "[1301, 2558]\n",
      "1.074\n",
      "multitask_size_base_epochs_100_patience_10_state_0.5_all_text_seed_100\n",
      "[4292, 4302]\n",
      "[562, 2558]\n",
      "1.085\n",
      "multitask_size_base_epochs_100_patience_10_state_0.5_random_text_seed_100\n",
      "[4129, 4302]\n",
      "[1576, 2558]\n",
      "1.063\n",
      "multitask_size_base_epochs_100_patience_10_state_0.5_targeted_text_seed_100\n",
      "[3614, 4302]\n",
      "[1224, 2558]\n",
      "1.075\n",
      "multitask_size_base_epochs_100_patience_10_state_0.6_all_text_seed_100\n",
      "[4290, 4302]\n",
      "[357, 2558]\n",
      "1.086\n",
      "multitask_size_base_epochs_100_patience_10_state_0.6_random_text_seed_100\n",
      "[4103, 4302]\n",
      "[1639, 2558]\n",
      "1.071\n",
      "multitask_size_base_epochs_100_patience_10_state_0.6_targeted_text_seed_100\n",
      "[3863, 4302]\n",
      "[1506, 2558]\n",
      "1.071\n",
      "multitask_size_base_epochs_100_patience_10_state_0.7_random_text_seed_100\n",
      "[3975, 4302]\n",
      "[1685, 2558]\n",
      "1.069\n",
      "multitask_size_base_epochs_100_patience_10_state_0.7_targeted_text_seed_100\n",
      "[3887, 4302]\n",
      "[1562, 2558]\n",
      "1.065\n",
      "multitask_size_base_epochs_100_patience_10_state_0.8_all_text_seed_100\n",
      "[4282, 4302]\n",
      "[548, 2558]\n",
      "1.090\n",
      "multitask_size_base_epochs_100_patience_10_state_0.8_random_text_seed_100\n",
      "[4046, 4302]\n",
      "[1774, 2558]\n",
      "1.065\n",
      "multitask_size_base_epochs_100_patience_10_state_0.8_targeted_text_seed_100\n",
      "[3774, 4302]\n",
      "[1641, 2558]\n",
      "1.068\n",
      "multitask_size_base_epochs_100_patience_10_state_0.9_all_text_seed_100\n",
      "[4277, 4302]\n",
      "[1229, 2558]\n",
      "1.085\n",
      "multitask_size_base_epochs_100_patience_10_state_0.9_random_text_seed_100\n",
      "[4009, 4302]\n",
      "[1751, 2558]\n",
      "1.087\n",
      "multitask_size_base_epochs_100_patience_10_state_0.9_targeted_text_seed_100\n",
      "[3467, 4302]\n",
      "[1620, 2558]\n",
      "1.077\n",
      "multitask_size_base_epochs_100_patience_10_state_1.0_all_text_seed_100\n",
      "[4238, 4302]\n",
      "[1688, 2558]\n",
      "1.086\n",
      "multitask_size_base_epochs_100_patience_10_state_1.0_random_text_seed_100\n",
      "[4038, 4302]\n",
      "[1819, 2558]\n",
      "1.060\n",
      "multitask_size_base_epochs_100_patience_10_state_1.0_targeted_text_seed_100\n",
      "[3389, 4302]\n",
      "[1675, 2558]\n",
      "1.076\n"
     ]
    }
   ],
   "source": [
    "total_corr_list = []\n",
    "perplexity_list = []\n",
    "for model_name in model_names:\n",
    "    model_path = path.join(base_dir, model_name)\n",
    "    log_file = path.join(model_path, 'dev.jsonl')\n",
    "    if path.exists(log_file):        \n",
    "        print(model_name)\n",
    "        model = torch.load(path.join(path.join(model_path, \"best\"), \"model.pt\"))\n",
    "        best_val_loss = model[\"train_info\"][\"best_val_loss\"]\n",
    "        \n",
    "        data = json.load(open(log_file))\n",
    "        \n",
    "        orig = [0, 0]\n",
    "        chng = [0, 0]\n",
    "        for key in data:\n",
    "            instance = data[key]\n",
    "            for output in instance[\"output\"]:\n",
    "                if output[\"same_as_init\"]:\n",
    "                    orig[1] += 1\n",
    "                    if output[\"corr\"]:\n",
    "                        orig[0] += 1\n",
    "                else:\n",
    "                    chng[1] += 1\n",
    "                    if output[\"corr\"]:\n",
    "                        chng[0] += 1\n",
    "                        \n",
    "        print(orig)\n",
    "        print(chng)\n",
    "        print(f\"{best_val_loss:.3f}\")\n",
    "        \n",
    "        \n",
    "        total_corr = orig[0] + chng[0]\n",
    "        perplexity = math.exp(best_val_loss)\n",
    "        \n",
    "        if 'oracle' not in model_name:\n",
    "            total_corr_list.append(total_corr)\n",
    "            perplexity_list.append(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4299, 5064, 3995, 4310, 5352, 4867, 4437, 5061, 5070, 4712, 5337, 5275, 4854, 5705, 4838, 4647, 5742, 5369, 5660, 5449, 4830, 5820, 5415, 5506, 5760, 5087, 5926, 5857, 5064]\n",
      "[2.952305592428037, 2.9148687810569305, 2.9341117577149083, 2.9587728258159367, 2.9278556493913936, 2.9408890704681667, 2.9579420984611566, 2.937825058155326, 2.903395200784267, 2.9628267951937586, 2.9340183615045063, 2.928060342579809, 2.9600942208294607, 2.896477867115539, 2.928828532393884, 2.9629714061367856, 2.917062703279541, 2.91736969827477, 2.9115276445268004, 2.900587496407705, 2.9749525936088457, 2.8996187376632387, 2.910548202184257, 2.9595443601241707, 2.9664389738700767, 2.9368290969803215, 2.9619736935424426, 2.887121319832514, 2.931524005221447]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=-0.48084739864669007, pvalue=0.008279017247201098)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(total_corr_list)\n",
    "print(perplexity_list)\n",
    "spearmanr(total_corr_list, perplexity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.146, 0.134, 0.123, 0.132, 0.138, 0.135, 0.145, 0.135, 0.172, 0.165, 0.134, 0.166, 0.146, 0.128, 0.184, 0.134, 0.14, 0.158, 0.129, 0.136, 0.17, 0.138, 0.128, 0.191, 0.15, 0.176, 0.158, 0.123, 0.184, 0.188, 0.16]\n",
      "[2.9540370418473385, 2.952305592428037, 2.9148687810569305, 2.9341117577149083, 2.9587728258159367, 2.9278556493913936, 2.9408890704681667, 2.9579420984611566, 2.937825058155326, 2.903395200784267, 2.9628267951937586, 2.9340183615045063, 2.928060342579809, 2.9600942208294607, 2.896477867115539, 2.928828532393884, 2.9629714061367856, 2.917062703279541, 2.91736969827477, 2.956082782074605, 2.9115276445268004, 2.900587496407705, 2.9749525936088457, 2.8996187376632387, 2.910548202184257, 2.9595443601241707, 2.9664389738700767, 2.9368290969803215, 2.9619736935424426, 2.887121319832514, 2.931524005221447]\n",
      "31\n",
      "0.191\n"
     ]
    }
   ],
   "source": [
    "perplexity_list = []\n",
    "mrr_list = []\n",
    "\n",
    "model_names = [\n",
    "    \"size_base_epochs_100_patience_10_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.1_all_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.1_random_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.1_targeted_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.2_all_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.2_random_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.2_targeted_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.3_all_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.3_random_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.3_targeted_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.4_all_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.4_random_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.4_targeted_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.5_all_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.5_random_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.5_targeted_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.6_all_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.6_random_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.6_targeted_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.7_all_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.7_random_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.7_targeted_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.8_all_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.8_random_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.8_targeted_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.9_all_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.9_random_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.9_targeted_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_1.0_all_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_1.0_random_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_1.0_targeted_text_seed_100\", \n",
    "]\n",
    "\n",
    "model_names = ['multitask_' + model_name for model_name in model_names]\n",
    "\n",
    "for model_name in model_names:\n",
    "    model_path = path.join(base_dir, model_name)\n",
    "    \n",
    "    model = torch.load(path.join(path.join(model_path, \"best\"), \"model.pt\"))\n",
    "    best_val_loss = model[\"train_info\"][\"best_val_loss\"]\n",
    "        \n",
    "    \n",
    "    log_file = path.join(model_path, 'dev.jsonl')\n",
    "    cloze_mrr_file = path.join(model_path, 'cloze_mrr.txt')\n",
    "    \n",
    "#     if path.exists(log_file):        \n",
    "    cloze_mrr = float(open(cloze_mrr_file).read())\n",
    "    perplexity = math.exp(best_val_loss)\n",
    "\n",
    "    perplexity_list.append(perplexity)\n",
    "    mrr_list.append(cloze_mrr)\n",
    "#         print(cloze_mrr)\n",
    "\n",
    "print(mrr_list)\n",
    "print(perplexity_list)\n",
    "\n",
    "\n",
    "print(len(perplexity_list))\n",
    "print(max(mrr_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpearmanrResult(correlation=-0.31708567322498643, pvalue=0.08219697379057851)\n",
      "2.887121319832514\n",
      "0.191\n"
     ]
    }
   ],
   "source": [
    "print(spearmanr(perplexity_list, mrr_list))\n",
    "print(min(perplexity_list))\n",
    "print(max(mrr_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.887121319832514, 2.896477867115539, 2.8996187376632387, 2.900587496407705, 2.903395200784267, 2.910548202184257, 2.9115276445268004, 2.9148687810569305, 2.917062703279541, 2.91736969827477, 2.9278556493913936, 2.928060342579809, 2.928828532393884, 2.931524005221447, 2.9340183615045063, 2.9341117577149083, 2.9368290969803215, 2.937825058155326, 2.9408890704681667, 2.952305592428037, 2.9540370418473385, 2.956082782074605, 2.9579420984611566, 2.9587728258159367, 2.9595443601241707, 2.9600942208294607, 2.9619736935424426, 2.9628267951937586, 2.9629714061367856, 2.9664389738700767, 2.9749525936088457]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(perplexity_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4299, 5064, 3995, 4310, 5352, 4867, 4437, 5061, 5070, 4712, 5337, 5275, 4854, 5705, 4838, 4647, 5742, 5369, 5660, 5449, 4830, 5820, 5415, 5506, 5760, 5087, 5926, 5857, 5064]\n",
      "[0.134, 0.123, 0.132, 0.138, 0.135, 0.145, 0.135, 0.172, 0.165, 0.134, 0.166, 0.146, 0.128, 0.184, 0.134, 0.14, 0.158, 0.129, 0.17, 0.138, 0.128, 0.191, 0.15, 0.176, 0.158, 0.123, 0.184, 0.188, 0.16]\n",
      "[3995, 4299, 4310, 4437, 4647, 4712, 4830, 4838, 4854, 4867, 5061, 5064, 5064, 5070, 5087, 5275, 5337, 5352, 5369, 5415, 5449, 5506, 5660, 5705, 5742, 5760, 5820, 5857, 5926]\n"
     ]
    }
   ],
   "source": [
    "mrr_list = []\n",
    "total_corr_list = []\n",
    "perplexity_list = []\n",
    "\n",
    "model_names = [\n",
    "    \"size_base_epochs_100_patience_10_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.1_all_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.1_random_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.1_targeted_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.2_all_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.2_random_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.2_targeted_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.3_all_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.3_random_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.3_targeted_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.4_all_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.4_random_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.4_targeted_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.5_all_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.5_random_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.5_targeted_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.6_all_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.6_random_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.6_targeted_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.7_all_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.7_random_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.7_targeted_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.8_all_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.8_random_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.8_targeted_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.9_all_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.9_random_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_0.9_targeted_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_1.0_all_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_1.0_random_text_seed_100\", \n",
    "    \"size_base_epochs_100_patience_10_state_1.0_targeted_text_seed_100\", \n",
    "]\n",
    "\n",
    "model_names = ['multitask_' + model_name for model_name in model_names]\n",
    "\n",
    "for model_name in model_names:\n",
    "    model_path = path.join(base_dir, model_name)\n",
    "#     if 'random' in model_name:\n",
    "#         continue\n",
    "    model = torch.load(path.join(path.join(model_path, \"best\"), \"model.pt\"))\n",
    "    best_val_loss = model[\"train_info\"][\"best_val_loss\"]\n",
    "    log_file = path.join(model_path, 'dev.jsonl')\n",
    "    if path.exists(log_file):        \n",
    "        data = json.load(open(log_file))        \n",
    "        orig = [0, 0]\n",
    "        chng = [0, 0]\n",
    "        for key in data:\n",
    "            instance = data[key]\n",
    "            for output in instance[\"output\"]:\n",
    "                if output[\"same_as_init\"]:\n",
    "                    orig[1] += 1\n",
    "                    if output[\"corr\"]:\n",
    "                        orig[0] += 1\n",
    "                else:\n",
    "                    chng[1] += 1\n",
    "                    if output[\"corr\"]:\n",
    "                        chng[0] += 1\n",
    "                        \n",
    "#         print(orig)\n",
    "#         print(chng)\n",
    "#         print(f\"{best_val_loss:.3f}\")\n",
    "        \n",
    "        \n",
    "        total_corr = orig[0] + chng[0]\n",
    "        total_corr_list.append(total_corr)\n",
    "    \n",
    "        cloze_mrr_file = path.join(model_path, 'cloze_mrr.txt')\n",
    "\n",
    "    #     if path.exists(log_file):        \n",
    "        cloze_mrr = float(open(cloze_mrr_file).read())\n",
    "        perplexity = math.exp(best_val_loss)\n",
    "\n",
    "        perplexity_list.append(best_val_loss)\n",
    "        mrr_list.append(cloze_mrr)\n",
    "#         print(cloze_mrr)\n",
    "\n",
    "print(total_corr_list)\n",
    "print(mrr_list)\n",
    "print(sorted(total_corr_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.653225214268353, pvalue=0.0001221771248509123)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanr(total_corr_list, mrr_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Egregious Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model_names = [\n",
    "# # #     'size_base_epochs_100_patience_10_state_0.1_targeted_text_seed_60',\n",
    "# #     'size_base_epochs_100_patience_10_state_0.5_targeted_text_seed_70',\n",
    "\n",
    "# #     'size_base_epochs_100_patience_10_state_0.1_all_text_seed_70',\n",
    "# #     'size_base_epochs_100_patience_10_state_0.25_all_text_seed_70',\n",
    "# #     'size_base_epochs_100_patience_10_state_0.5_all_text_seed_70',\n",
    "# #     'size_base_epochs_100_patience_10_state_0.75_all_text_seed_70',\n",
    "\n",
    "# # #     'size_base_epochs_100_patience_10_state_0.1_random_text_seed_60',\n",
    "# # #     'size_base_epochs_100_patience_10_state_0.25_random_text_seed_60',\n",
    "# # #     'size_base_epochs_100_patience_10_state_0.5_random_text_seed_60',\n",
    "# # #     'size_base_epochs_100_patience_10_state_0.75_random_text_seed_60',\n",
    "# # ]\n",
    "\n",
    "\n",
    "# for model_name in model_names:\n",
    "#     model_path = path.join(base_dir, model_name)\n",
    "#     log_file = path.join(model_path, 'dev.jsonl')\n",
    "#     if path.exists(log_file):        \n",
    "# #         print(model_name)\n",
    "#         model = torch.load(path.join(path.join(model_path, \"best\"), \"model.pt\"))\n",
    "#         best_val_loss = model[\"train_info\"][\"best_val_loss\"]\n",
    "        \n",
    "#         data = json.load(open(log_file))\n",
    "        \n",
    "#         error_count_dict = Counter()\n",
    "#         for key in data:\n",
    "#             instance = data[key]\n",
    "#             error_count = 0\n",
    "#             for output in instance[\"output\"]:\n",
    "#                 if not output[\"corr\"]:\n",
    "#                     error_count += 1\n",
    "            \n",
    "# #             if error_count == 6:\n",
    "# #                 print(instance)\n",
    "#             error_count_dict[error_count] += 1\n",
    "            \n",
    "# #         print(error_count_dict)\n",
    "# #         print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rap_nlp] *",
   "language": "python",
   "name": "conda-env-rap_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
